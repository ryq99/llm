# LARGE LANGUAGE MODEL
### Transformer Architecture
[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)  
[BLOOM: BigScience 176B Model](https://arxiv.org/pdf/2211.05100.pdf)  

<br>

### Pre-training and scaling laws
[Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)

<br>

### Model architectures and pre-training objectives
[What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?](https://arxiv.org/pdf/2204.05832.pdf)  
[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf) 

<br>

### Scaling laws and compute-optimal models
[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)  
[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  
[BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)  

<br>

### Instruction Finetuning
[Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)  
[Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)  

<br>

### Parameter- efficient fine tuning (PEFT)
[Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf)
[On the Effectiveness of Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2211.15583.pdf)
#### LoRA  
[LoRA Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)  
[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)  
#### Prompt Tuning  
[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)  

<br>

### Model Evaluation Metrics
[Holistic Evaluation of Language Model](https://crfm.stanford.edu/helm/latest/?scenarios=1)  
[General Language Understanding Evaluation (GLUE) benchmark](https://openreview.net/pdf?id=rJ4km2R5t7)  
[SuperGLUE](https://super.gluebenchmark.com/)  
[ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)  
[Measuring Massive Multitask Language Understanding (MMLU)](https://arxiv.org/pdf/2009.03300.pdf)  
[BigBench-Hard - Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models](https://arxiv.org/pdf/2206.04615.pdf)  